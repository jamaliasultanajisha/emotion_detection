{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport librosa\nimport wave\nimport keras\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import *\nfrom keras.optimizers import rmsprop\nimport seaborn as sn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\nimport sys\nimport glob\nimport math\nfrom scipy.fftpack import fft\nfrom scipy.fftpack.realtransforms import dct\nfrom scipy.signal import lfilter, hamming\nfrom imblearn.over_sampling import SMOTE\nfrom keras.optimizers import SGD\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils import class_weight\n\neps = 0.00000001\nprint (\"setup complete\")","execution_count":6,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"setup complete\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_mfcc(wav_file_name):\n    '''This function extracts mfcc features and obtain the mean of each dimension\n    Input : path_to_wav_file\n    Output: mfcc_features'''\n    y, sr = librosa.load(wav_file_name, res_type='kaiser_best', duration=3, offset=0.5)\n#     trimmed_data = np.zeros((160, 20))\n    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0)\n#     data = np.array(librosa.feature.mfcc(y = y, sr = sr, n_mfcc=40).T)\n#     if data.shape[0] <= 160:\n#         trimmed_data[:data.shape[0],0:] = data[:,0:]\n#     else:\n#         trimmed_data[0:,0:] = data[0:160,0:]\n    return mfccs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def genderClassification(item):\n    global gender_list\n    if int(item[18:-4]) % 2 == 0:\n        gender_list = 1  # female\n    if int(item[18:-4]) % 2 == 1: \n        gender_list = 0  # male \n    return gender_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def genderClassification_another(item):\n    global gender_list\n    if item[0] == 'f':\n        gender_list = 1  # female\n    if item[0] == 'm': \n        gender_list = 0  # male \n    return gender_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def classification(item):\n    global feeling_list\n    if item[11:14] == 'ANG':\n        feeling_list = 4  \n    if item[11:14] == 'DIS': \n        feeling_list = 6\n    if item[11:14] == 'FEA':\n        feeling_list = 5\n    if item[11:14] == 'HAP':\n        feeling_list = 2\n#     if item[11:14] == 'NEU':\n#         feeling_list = 0\n    if item[11:14] == 'SAD':\n        feeling_list = 3\n    return feeling_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ### although, we load the data here, it is not used in training or validation\n# for dirname, _, filenames in os.walk('/kaggle/input/audiowav-cremad/AudioWAV_cremad/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\nroot_dir = \"/kaggle/input/audiowav-cremad/AudioWAV_cremad/\" \n\nwithout_gender_cremad_only_data = [] ###stores the mfcc data\nwithout_gender_cremad_only_labels = [] ###stores the labels\nfor subdirs, dirs, files in os.walk(root_dir):\n    for file in files:\n        if file[11:14] == 'NEU':\n            continue\n        else:\n            y, sr = librosa.load(os.path.join(subdirs,file), res_type='kaiser_best', \n                                 duration=librosa.get_duration(filename=os.path.join(subdirs,file)), offset=0.25)\n            mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0)\n            without_gender_cremad_only_data.append(mfccs)\n            without_gender_cremad_only_labels.append(classification(file))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nwithout_gender_cremad_only_data_array = np.array(without_gender_cremad_only_data)\nwithout_gender_cremad_only_labels_array = np.array(without_gender_cremad_only_labels)\nwithout_gender_cremad_only_data_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input/ravdessaudiofilessfromvideo/ravdess_audio_files_from_video/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n### extract audio data from AV RAVDESS data\nroot_dir = \"/kaggle/input/ravdessaudiofilessfromvideo/ravdess_audio_files_from_video/\" \n\nwithout_gender_audio_only_data = []  # stores the mfcc data\nwithout_gender_audio_only_labels = []  # stores the labels\nfor subdirs, dirs, files in os.walk(root_dir):\n    for file in files:\n        if int(file[7:8]) == 1:\n                continue\n        if int(file[7:8]) == 2:\n            continue\n        else:\n            y, sr = librosa.load(os.path.join(subdirs, file), res_type='kaiser_best', \n                                 duration=librosa.get_duration(filename=os.path.join(subdirs,file)), offset=0.5)\n            mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n            without_gender_audio_only_data.append(mfccs)\n            without_gender_audio_only_labels.append(int(file[7:8]) - 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nwithout_gender_audio_only_data_array = np.array(without_gender_audio_only_data)\nwithout_gender_audio_only_labels_array = np.array(without_gender_audio_only_labels)\nwithout_gender_audio_only_data_array.shape\nwithout_gender_audio_only_labels_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### load data from savee dataset\n#### although, we load the data here, it is not used in training or validation\n# for dirname, _, filenames in os.walk('/kaggle/input/savee-database/audiodata/AudioData/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\nroot_dir = \"/kaggle/input/savee-database/audiodata/AudioData/\"\n# root_dir = \"../input/audio_speech_actors_01-24/\"\nwithout_gender_savee_data = []\nwithout_gender_savee_labels = []\nfor actor_dir in sorted(os.listdir(root_dir)):\n    if actor_dir[-4:] == \".txt\":\n        continue\n    for file_name in os.listdir(os.path.join(root_dir, actor_dir)):\n        if file_name[0] == \"c\":\n            continue\n        if file_name[0] == \"n\":\n            continue\n        wav_file_name = os.path.join(root_dir, actor_dir, file_name)\n        without_gender_savee_data.append(extract_mfcc(wav_file_name))\n        if file_name[0] == \"a\":\n            without_gender_savee_labels.append(4)\n        if file_name[0] == \"d\":\n            without_gender_savee_labels.append(6)\n        if file_name[0] == \"f\":\n            without_gender_savee_labels.append(5)\n        if file_name[0] == \"h\":\n            without_gender_savee_labels.append(2)\n        if file_name[:2] == \"sa\":\n            without_gender_savee_labels.append(3)\n        if file_name[:2] == \"su\":\n            without_gender_savee_labels.append(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nwithout_gender_savee_data_array = np.asarray(without_gender_savee_data)\nwithout_gender_savee_label_array = np.array(without_gender_savee_labels)\n# to_categorical(without_gender_savee_label_array)[0].shape\nwithout_gender_savee_data_array.shape\nwithout_gender_savee_label_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\nroot_dir = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\" \n##### load radvess speech data #####\n# root_dir = \"../input/audio_speech_actors_01-24/\"\n# root_dir = \"../input/audio_speech_actors_01-24/\"\n# actor_dir = os.listdir(\"../input/audio_speech_actors_01-24/\")\nwithout_gender_radvess_speech_labels = []\nwithout_gender_ravdess_speech_data = []\nfor actor_dir in sorted(os.listdir(root_dir)):\n    actor_name = os.path.join(root_dir, actor_dir)\n    for file in os.listdir(actor_name):\n        if int(file[7:8]) == 1:\n            continue\n        if int(file[7:8]) == 2:\n            continue\n        else:\n            without_gender_radvess_speech_labels.append(int(file[7:8]) - 1)\n        #         radvess_speech_labels.append(classification(file))\n            wav_file_name = os.path.join(root_dir, actor_dir, file)\n            without_gender_ravdess_speech_data.append(extract_mfcc(wav_file_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nwithout_gender_ravdess_speech_data_array = np.asarray(without_gender_ravdess_speech_data)\nwithout_gender_ravdess_speech_label_array = np.array(without_gender_radvess_speech_labels)\nwithout_gender_ravdess_speech_data_array.shape\nwithout_gender_ravdess_speech_label_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n### load RAVDESS song data\nroot_dir = \"../input/ravdess-emotional-song-audio/audio_song_actors_01-24/\"\nwithout_gender_radvess_song_labels = []\nwithout_gender_ravdess_song_data = []\nfor actor_dir in sorted(os.listdir(root_dir)):\n    actor_name = os.path.join(root_dir, actor_dir)\n    for file in os.listdir(actor_name):\n        if int(file[7:8]) == 1:\n            continue\n        if int(file[7:8]) == 2:\n            continue\n        else:\n            without_gender_radvess_song_labels.append(int(file[7:8]) - 1)\n        #         radvess_song_labels.append(classification(file))\n            wav_file_name = os.path.join(root_dir, actor_dir, file)\n            without_gender_ravdess_song_data.append(extract_mfcc(wav_file_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nwithout_gender_ravdess_song_data_array = np.asarray(without_gender_ravdess_song_data)\nwithout_gender_ravdess_song_label_array = np.array(without_gender_radvess_song_labels)\nwithout_gender_ravdess_song_label_array.shape\nwithout_gender_ravdess_song_data_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input/ravdess-audio-from-video-song/ravdess_audio_files_from_video_song/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n\nroot_dir = \"/kaggle/input/ravdess-audio-from-video-song/ravdess_audio_files_from_video_song/\" \n\nwithout_gender_audio_only_data_song = [] ###stores the mfcc data\nwithout_gender_audio_only_labels_song = [] ###stores the labels\nfor subdirs, dirs, files in os.walk(root_dir):\n    for file in files:\n        if int(file[7:8]) == 1:\n            continue\n        if int(file[7:8]) == 2:\n            continue\n        else:\n            y, sr = librosa.load(os.path.join(subdirs,file), res_type='kaiser_best', \n                                     duration=librosa.get_duration(filename=os.path.join(subdirs,file)), offset=0.5)\n            mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0)\n            without_gender_audio_only_data_song.append(mfccs)\n        #         audio_only_labels_song.append(classification(file))\n            without_gender_audio_only_labels_song.append(int(file[7:8]) - 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nwithout_gender_audio_only_data_song_array = np.array(without_gender_audio_only_data_song)\nwithout_gender_audio_only_labels_song_array = np.array(without_gender_audio_only_labels_song)\nwithout_gender_audio_only_data_song_array.shape\nwithout_gender_audio_only_labels_song_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #### combine data\nwithout_gender_data = np.r_[without_gender_audio_only_data_array, without_gender_cremad_only_data_array, without_gender_audio_only_data_song_array, without_gender_ravdess_song_data_array, without_gender_savee_data_array, without_gender_ravdess_speech_data_array]  # ravdess_song_data_array, audio_only_data_song_array,\nwithout_gender_labels = np.r_[without_gender_audio_only_labels_array, without_gender_cremad_only_labels_array, without_gender_audio_only_labels_song_array, without_gender_ravdess_song_label_array, without_gender_savee_label_array, without_gender_ravdess_speech_label_array]  # ravdess_song_label_array, audio_only_labels_song_array,\n# data = ravdess_speech_data_array\n# labels = ravdess_speech_label_array\nwithout_gender_labels.shape\nwithout_gender_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### make categorical labels\nwithout_gender_labels_categorical = to_categorical(without_gender_labels)\nwithout_gender_data.shape\nwithout_gender_labels_categorical.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def without_gender_create_model_LSTM():\n    ### LSTM model, referred to the model A in the report\n    model = Sequential()\n    model.add(LSTM(128, return_sequences=False, input_shape=(40, 1)))\n    model.add(Dense(64))\n#     model.add(Dropout(0.4))\n    model.add(Activation('relu'))\n    model.add(Dense(32))\n    model.add(Dropout(0.1))   ### 0.4\n    model.add(Activation('relu'))\n    model.add(Dense(8))\n    model.add(Activation('softmax'))\n    print (\"model A : \")\n    model.summary()\n    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n    return model\n\ndef create_model_CNN():\n    ### CNN model, referred to the model B in the report\n    model = Sequential()\n    model.add(Conv1D(8, kernel_size = 3, input_shape=(40, 1)))\n    model.add(Activation('relu'))\n    model.add(Conv1D(16,kernel_size = 3))\n    model.add(Activation('relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Conv1D(32, kernel_size = 3))\n    model.add(Activation('relu'))\n    model.add(Conv1D(16, kernel_size = 3))\n    model.add(Activation('relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(8))\n    model.add(Activation('softmax'))\n    print (\"model B : \")\n    model.summary()\n    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n    return model\n\ndef new_CNN():\n    ### CNN model, referred to the model C in the report\n    model = Sequential()\n    model.add(Conv1D(8, 5,padding='same', input_shape=(40, 1)))\n    model.add(Activation('relu'))\n    model.add(Conv1D(16, 5,padding='same'))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.1))\n    model.add(MaxPooling1D(pool_size=(8)))\n    model.add(Conv1D(32, 5,padding='same',))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(16, 5,padding='same',))\n    model.add(Activation('relu'))\n    model.add(Flatten())\n    model.add(Dense(8))\n    model.add(Activation('softmax'))\n    opt = rmsprop(lr=0.00001, decay=1e-6)\n    print (\"model C : \")\n    model.summary()\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n    return model\n\ndef train_CNN():\n    ### CNN model, referred to the model D in the report\n    model = Sequential()\n    model.add(Conv1D(128, 5,padding='same', input_shape=(40,1)))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.1))\n    model.add(MaxPooling1D(pool_size=(8)))\n    model.add(Conv1D(128, 5,padding='same',))\n    model.add(Activation('relu'))\n#     model.add(Dropout(0.1))\n    model.add(Flatten())\n    model.add(Dense(8))\n    model.add(Activation('softmax'))\n    opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)\n    print (\"model D : \")\n    model.summary()\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n    return model\n\ndef male_female_pitch():\n    model = Sequential()\n    model.add(Conv1D(256, 5, padding='same', input_shape=(40, 1)))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.1))\n    model.add(MaxPooling1D(pool_size=16))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.1))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Flatten())\n    model.add(Dense(8))\n    model.add(Activation('softmax'))\n    opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n    print (\"model E : \")\n    model.summary()\n    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n    return model\n\ndef emotion_acc():\n    model = Sequential()\n    model.add(Conv1D(256, 5, padding='same', input_shape=(40, 1)))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.1))\n    model.add(MaxPooling1D(pool_size=16))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.1))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Flatten())\n    model.add(Dense(8))\n    model.add(Activation('softmax'))\n    opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n    print (\"model F : \")\n    model.summary()\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.random.shuffle(data)\nwithout_gender_number_of_samples = without_gender_data.shape[0]\nwithout_gender_training_samples = int(without_gender_number_of_samples * 0.8)\nwithout_gender_validation_samples = int(without_gender_training_samples * 0.33)\nwithout_gender_test_samples = int(without_gender_number_of_samples-(without_gender_training_samples))\nwithout_gender_class_weights = class_weight.compute_class_weight('balanced', np.unique(without_gender_labels_categorical[without_gender_training_samples]),\n                                                  without_gender_labels_categorical[without_gender_training_samples])\nprint (without_gender_training_samples)\nprint (without_gender_validation_samples)\nprint (without_gender_test_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### train using model A\nmodel_without_gender = male_female_pitch()  # without_gender_create_model_LSTM()\nhistory = model_without_gender.fit(np.expand_dims(without_gender_data[:without_gender_training_samples],-1), \n                      without_gender_labels_categorical[:without_gender_training_samples], \n                      validation_data=(np.expand_dims(without_gender_data[without_gender_validation_samples:], -1), \n                      without_gender_labels_categorical[without_gender_validation_samples:]),\n                      class_weight=without_gender_class_weights, epochs=300, shuffle=False)  # class_weight=without_gender_class_weights, ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### loss plots using model A\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'ro', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### accuracy plots using model A\nplt.clf()                                                \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'ro', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"j = 1\nprint (\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        j + 1, 100, max(history.history['accuracy']), max(history.history['val_accuracy'])))\n# predictions = model_A.predict(np.expand_dims(data[test_samples:], -1))\n# predictions[0:5]\ntrain_acc = model_without_gender.evaluate(np.expand_dims(without_gender_data[:without_gender_training_samples], -1), without_gender_labels_categorical[:without_gender_training_samples], verbose=0)\ntest_acc = model_without_gender.evaluate(np.expand_dims(without_gender_data[without_gender_test_samples:], -1),\n                                without_gender_labels_categorical[without_gender_test_samples:], verbose=0)\nval_acc = model_without_gender.evaluate(np.expand_dims(without_gender_data[without_gender_validation_samples:], -1), without_gender_labels_categorical[without_gender_validation_samples:], verbose=0)\nprint (\"accuracy printing\")\nprint (train_acc)\nprint (test_acc)\nprint (val_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############With gender detection starts#######################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ### although, we load the data here, it is not used in training or validation\n# for dirname, _, filenames in os.walk('/kaggle/input/audiowav-cremad/AudioWAV_cremad/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\nroot_dir = \"/kaggle/input/audiowav-cremad/AudioWAV_cremad/\" \n\ncremad_only_data = [] ###stores the mfcc data\ncremad_only_labels = [] ###stores the labels\nfor subdirs, dirs, files in os.walk(root_dir):\n    for file in files:\n        if file[11:14] == 'NEU':\n            continue\n        else:\n            y, sr = librosa.load(os.path.join(subdirs,file), res_type='kaiser_best', \n                                 duration=librosa.get_duration(filename=os.path.join(subdirs,file)), offset=0.25)\n            mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0)\n            cremad_only_data.append(mfccs)\n            cremad_only_labels.append(genderClassification_another(file))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\ncremad_only_data_array = np.array(cremad_only_data)\ncremad_only_labels_array = np.array(cremad_only_labels)\ncremad_only_data_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### load data from savee dataset\n#### although, we load the data here, it is not used in training or validation\n# for dirname, _, filenames in os.walk('/kaggle/input/savee-database/audiodata/AudioData/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\nroot_dir = \"/kaggle/input/savee-database/audiodata/AudioData/\"\n# root_dir = \"../input/audio_speech_actors_01-24/\"\nsavee_data = []\nsavee_labels = []\nfor actor_dir in sorted(os.listdir(root_dir)):\n    if actor_dir[-4:] == \".txt\":\n        continue\n    for file_name in os.listdir(os.path.join(root_dir, actor_dir)):\n        if file_name[0] == \"c\":\n            continue\n        if file_name[0] == \"n\":\n            continue\n        else:\n    #         gender_list = 1\n            savee_labels.append(0)  # male\n            wav_file_name = os.path.join(root_dir, actor_dir, file_name)\n            savee_data.append(extract_mfcc(wav_file_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nsavee_data_array = np.asarray(savee_data)\nsavee_label_array = np.array(savee_labels)\nto_categorical(savee_label_array)[0].shape\n# savee_data_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\nroot_dir = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\" \n##### load radvess speech data #####\n# root_dir = \"../input/audio_speech_actors_01-24/\"\n# root_dir = \"../input/audio_speech_actors_01-24/\"\n# actor_dir = os.listdir(\"../input/audio_speech_actors_01-24/\")\nradvess_speech_labels = []\nravdess_speech_data = []\nfor actor_dir in sorted(os.listdir(root_dir)):\n    actor_name = os.path.join(root_dir, actor_dir)\n    for file in os.listdir(actor_name):\n        if int(file[7:8]) == 1:\n            continue\n        if int(file[7:8]) == 2:\n            continue\n        else:\n            radvess_speech_labels.append(genderClassification(file))\n            wav_file_name = os.path.join(root_dir, actor_dir, file)\n            ravdess_speech_data.append(extract_mfcc(wav_file_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nravdess_speech_data_array = np.asarray(ravdess_speech_data)\nravdess_speech_label_array = np.array(radvess_speech_labels)\nravdess_speech_label_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input/ravdessaudiofilessfromvideo/ravdess_audio_files_from_video/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n### extract audio data from AV RAVDESS data\nroot_dir = \"/kaggle/input/ravdessaudiofilessfromvideo/ravdess_audio_files_from_video/\" \n\naudio_only_data = [] ###stores the mfcc data\naudio_only_labels = [] ###stores the labels\nfor subdirs, dirs, files in os.walk(root_dir):\n    for file in files:\n        if int(file[7:8]) == 1:\n            continue\n        if int(file[7:8]) == 2:\n            continue\n        else:\n            y, sr = librosa.load(os.path.join(subdirs,file), res_type='kaiser_best', \n                                 duration=librosa.get_duration(filename=os.path.join(subdirs,file)), offset=0.5)\n            mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0)\n            audio_only_data.append(mfccs)\n            audio_only_labels.append(genderClassification(file))\n    #         audio_only_labels.append(int(file[7:8]) - 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\naudio_only_data_array = np.array(audio_only_data)\naudio_only_labels_array = np.array(audio_only_labels)\naudio_only_data_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input/ravdess-audio-from-video-song/ravdess_audio_files_from_video_song/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n### extract audio data from AV RAVDESS data\nroot_dir = \"/kaggle/input/ravdess-audio-from-video-song/ravdess_audio_files_from_video_song/\" \n\naudio_only_data_song = [] ###stores the mfcc data\naudio_only_labels_song = [] ###stores the labels\nfor subdirs, dirs, files in os.walk(root_dir):\n    for file in files:\n        if int(file[7:8]) == 1:\n            continue\n        if int(file[7:8]) == 2:\n            continue\n        else:\n            y, sr = librosa.load(os.path.join(subdirs,file), res_type='kaiser_best', \n                                 duration=librosa.get_duration(filename=os.path.join(subdirs,file)), offset=0.5)\n            mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0)\n            audio_only_data_song.append(mfccs)\n            audio_only_labels_song.append(genderClassification(file))\n    #         audio_only_labels_song.append(int(file[7:8]) - 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\naudio_only_data_song_array = np.array(audio_only_data_song)\naudio_only_labels_song_array = np.array(audio_only_labels_song)\naudio_only_data_song_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n### load RAVDESS song data\nroot_dir = \"../input/ravdess-emotional-song-audio/audio_song_actors_01-24/\"\nradvess_song_labels = []\nravdess_song_data = []\nfor actor_dir in sorted(os.listdir(root_dir)):\n    actor_name = os.path.join(root_dir, actor_dir)\n    for file in os.listdir(actor_name):\n        if int(file[7:8]) == 1:\n            continue\n        if int(file[7:8]) == 2:\n            continue\n        else:\n    #         radvess_song_labels.append(int(file[7:8]) - 1)\n            radvess_song_labels.append(genderClassification(file))\n            wav_file_name = os.path.join(root_dir, actor_dir, file)\n            ravdess_song_data.append(extract_mfcc(wav_file_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nravdess_song_data_array = np.asarray(ravdess_song_data)\nravdess_song_label_array = np.array(radvess_song_labels)\nravdess_song_label_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #### combine data\ndata = np.r_[audio_only_data_array, cremad_only_data_array, ravdess_song_data_array, savee_data_array, audio_only_data_song_array, ravdess_speech_data_array]  # ravdess_song_data_array, audio_only_data_song_array,\nlabels = np.r_[audio_only_labels_array, cremad_only_labels_array, ravdess_song_label_array, savee_label_array, audio_only_labels_song_array, ravdess_speech_label_array]  # ravdess_song_label_array, audio_only_labels_song_array,\n# data = ravdess_speech_data_array\n# labels = ravdess_speech_label_array\nlabels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### make categorical labels\nlabels_categorical = to_categorical(labels)\ndata.shape\nlabels_categorical.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_LSTM():\n    ### LSTM model, referred to the model A in the report\n    model = Sequential()\n    model.add(LSTM(128, return_sequences=False, input_shape=(40, 1)))\n    model.add(Dense(64))\n    model.add(Dropout(0.4))\n    model.add(Activation('relu'))\n    model.add(Dense(32))\n    model.add(Dropout(0.4))   ### 0.4\n    model.add(Activation('relu'))\n    model.add(Dense(2))\n    model.add(Activation('softmax'))\n    print (\"model A : \")\n    model.summary()\n    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n    return model\n\ndef male_female_pitch():\n    model = Sequential()\n    model.add(Conv1D(256, 5, padding='same', input_shape=(40, 1)))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.1))\n    model.add(MaxPooling1D(pool_size=16))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.1))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Flatten())\n    model.add(Dense(2))\n    model.add(Activation('softmax'))\n    opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n    print (\"model B : \")\n    model.summary()\n    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n    return model","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.random.shuffle(data)\nnumber_of_samples = data.shape[0]\ntraining_samples = int(number_of_samples * 0.8)\nvalidation_samples = int(training_samples * 0.33)\ntest_samples = int(number_of_samples-(training_samples))\nprint (training_samples)\nprint (validation_samples)\nprint (test_samples)\n# class_weights = class_weight.compute_class_weight('balanced', np.unique(labels_categorical[training_samples]),\n#                                                   labels_categorical[training_samples])\n# male_female_pitch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_B = create_model_LSTM()  # male_female_pitch()  # create_model_LSTM()\nhistory = model_B.fit(np.expand_dims(data[:training_samples],-1), \n                      labels_categorical[:training_samples], \n                      validation_data=(np.expand_dims(data[validation_samples:], -1), \n                                       labels_categorical[validation_samples:]), \n                      epochs=300, shuffle=False)  # class_weight=class_weights, ","execution_count":10,"outputs":[{"output_type":"stream","text":"model A : \nModel: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_2 (LSTM)                (None, 128)               66560     \n_________________________________________________________________\ndense_4 (Dense)              (None, 64)                8256      \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 64)                0         \n_________________________________________________________________\nactivation_4 (Activation)    (None, 64)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 32)                0         \n_________________________________________________________________\nactivation_5 (Activation)    (None, 32)                0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 2)                 66        \n_________________________________________________________________\nactivation_6 (Activation)    (None, 2)                 0         \n=================================================================\nTotal params: 76,962\nTrainable params: 76,962\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"<keras.engine.sequential.Sequential at 0x7f65352db860>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"### loss plots using model A\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'ro', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### accuracy plots using model A\nplt.clf()                                                \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'ro', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"j = 1\nprint (\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        j + 1, 100, max(history.history['accuracy']), max(history.history['val_accuracy'])))\nprint (\"CNN {0:d}: Epochs={1:d}, Train loss={2:.5f}, Validation loss={3:.5f}\".format(\n        j + 1, 100, min(history.history['loss']), min(history.history['val_loss'])))\n# predictions = model_A.predict(np.expand_dims(data[test_samples:], -1))\n# predictions[0:5]\ntrain_acc = model_B.evaluate(np.expand_dims(data[:training_samples], -1), labels_categorical[:training_samples], verbose=0)\ntest_acc = model_B.evaluate(np.expand_dims(data[test_samples:], -1),\n                                labels_categorical[test_samples:], verbose=0)\nval_acc = model_B.evaluate(np.expand_dims(data[validation_samples:], -1), labels_categorical[validation_samples:], verbose=0)\nprint (\"accuracy printing\")\nprint (train_acc)\nprint (test_acc)\nprint (val_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################female gender emotion#######################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ### although, we load the data here, it is not used in training or validation\n# for dirname, _, filenames in os.walk('/kaggle/input/audiowav-cremad/AudioWAV_cremad/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\nroot_dir = \"/kaggle/input/audiowav-cremad/AudioWAV_cremad/\" \n\nfemale_cremad_only_data = [] ###stores the mfcc data\nfemale_cremad_only_labels = [] ###stores the labels\nfor subdirs, dirs, files in os.walk(root_dir):\n    for file in files:\n        if file[0] == 'f':\n            if file[11:14] == 'NEU':\n                continue\n            else:\n                y, sr = librosa.load(os.path.join(subdirs,file), res_type='kaiser_best', \n                                         duration=librosa.get_duration(filename=os.path.join(subdirs,file)),\n                                         offset=0.25)\n                mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0)\n                female_cremad_only_data.append(mfccs)\n                female_cremad_only_labels.append(classification(file))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nfemale_cremad_only_data_array = np.array(female_cremad_only_data)\nfemale_cremad_only_labels_array = np.array(female_cremad_only_labels)\nfemale_cremad_only_data_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input/ravdessaudiofilessfromvideo/ravdess_audio_files_from_video/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n### extract audio data from AV RAVDESS data\nroot_dir = \"/kaggle/input/ravdessaudiofilessfromvideo/ravdess_audio_files_from_video/\" \n\nfemale_audio_only_data = []  # stores the mfcc data\nfemale_audio_only_labels = []  # stores the labels\nfor subdirs, dirs, files in os.walk(root_dir):\n    for file in files:\n        if int(file[18:-4]) % 2 == 0:\n            if int(file[7:8]) == 1:\n                continue\n            if int(file[7:8]) == 2:\n                continue\n            else:\n                y, sr = librosa.load(os.path.join(subdirs, file), res_type='kaiser_best', \n                                     duration=librosa.get_duration(filename=os.path.join(subdirs,file)),\n                                     offset=0.5)\n                mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n                female_audio_only_data.append(mfccs)\n                female_audio_only_labels.append(int(file[7:8]) - 1)\n                #         female_audio_only_labels.append(classification(file))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nfemale_audio_only_data_array = np.array(female_audio_only_data)\nfemale_audio_only_labels_array = np.array(female_audio_only_labels)\nfemale_audio_only_data_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\nroot_dir = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\" \n##### load radvess speech data #####\n# root_dir = \"../input/audio_speech_actors_01-24/\"\n# root_dir = \"../input/audio_speech_actors_01-24/\"\n# actor_dir = os.listdir(\"../input/audio_speech_actors_01-24/\")\nfemale_radvess_speech_labels = []\nfemale_ravdess_speech_data = []\nfor actor_dir in sorted(os.listdir(root_dir)):\n    actor_name = os.path.join(root_dir, actor_dir)\n    for file in os.listdir(actor_name):\n        if int(file[18:-4]) % 2 == 0:\n            if int(file[7:8]) == 1:\n                continue\n            if int(file[7:8]) == 2:\n                continue\n            else:\n                female_radvess_speech_labels.append(int(file[7:8]) - 1)\n        #         female_radvess_speech_labels.append(classification(file))\n                wav_file_name = os.path.join(root_dir, actor_dir, file)\n                female_ravdess_speech_data.append(extract_mfcc(wav_file_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nfemale_ravdess_speech_data_array = np.asarray(female_ravdess_speech_data)\nfemale_ravdess_speech_label_array = np.array(female_radvess_speech_labels)\nfemale_ravdess_speech_label_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n### load RAVDESS song data\nroot_dir = \"../input/ravdess-emotional-song-audio/audio_song_actors_01-24/\"\nfemale_radvess_song_labels = []\nfemale_ravdess_song_data = []\nfor actor_dir in sorted(os.listdir(root_dir)):\n    actor_name = os.path.join(root_dir, actor_dir)\n    for file in os.listdir(actor_name):\n        if int(file[18:-4]) % 2 == 0:\n            if int(file[7:8]) == 1:\n                continue\n            if int(file[7:8]) == 2:\n                continue\n            else:\n                female_radvess_song_labels.append(int(file[7:8]) - 1)\n        #         female_radvess_song_labels.append(classification(file))\n                wav_file_name = os.path.join(root_dir, actor_dir, file)\n                female_ravdess_song_data.append(extract_mfcc(wav_file_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nfemale_ravdess_song_data_array = np.asarray(female_ravdess_song_data)\nfemale_ravdess_song_label_array = np.array(female_radvess_song_labels)\nfemale_ravdess_song_label_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input/ravdess-audio-from-video-song/ravdess_audio_files_from_video_song/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nroot_dir = \"/kaggle/input/ravdess-audio-from-video-song/ravdess_audio_files_from_video_song/\" \n\nfemale_audio_only_data_song = [] ###stores the mfcc data\nfemale_audio_only_labels_song = [] ###stores the labels\nfor subdirs, dirs, files in os.walk(root_dir):\n    for file in files:\n        if int(file[18:-4]) % 2 == 0:\n            if int(file[7:8]) == 1:\n                continue\n            if int(file[7:8]) == 2:\n                continue\n            else:\n                y, sr = librosa.load(os.path.join(subdirs,file), res_type='kaiser_best', \n                                         duration=librosa.get_duration(filename=os.path.join(subdirs,file)), \n                                     offset=0.5)\n                mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0)\n                female_audio_only_data_song.append(mfccs)\n            #         female_audio_only_labels_song.append(classification(file))\n                female_audio_only_labels_song.append(int(file[7:8]) - 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nfemale_audio_only_data_song_array = np.array(female_audio_only_data_song)\nfemale_audio_only_labels_song_array = np.array(female_audio_only_labels_song)\nfemale_audio_only_data_song_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #### combine data\nfemale_data = np.r_[female_audio_only_data_array, female_cremad_only_data_array, female_ravdess_song_data_array, female_audio_only_data_song_array, female_ravdess_speech_data_array]  # savee_data_array,--> only male # ravdess_song_data_array, audio_only_data_song_array,\nfemale_labels = np.r_[female_audio_only_labels_array, female_cremad_only_labels_array, female_ravdess_song_label_array, female_audio_only_labels_song_array, female_ravdess_speech_label_array]  # savee_label_array,--> only male #ravdess_song_label_array, audio_only_labels_song_array,\n# data = ravdess_speech_data_array\n# labels = ravdess_speech_label_array\nfemale_data.shape\nfemale_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### make categorical labels\nfemale_labels_categorical = to_categorical(female_labels)\nfemale_data.shape\nfemale_labels_categorical.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def female_create_model_LSTM():\n    ### LSTM model, referred to the model A in the report\n    model = Sequential()\n    model.add(LSTM(128, return_sequences=False, input_shape=(40, 1)))\n    model.add(Dense(64))\n    model.add(Dropout(0.4))\n    model.add(Activation('relu'))\n    model.add(Dense(32))\n    model.add(Dropout(0.4))   ### 0.4\n    model.add(Activation('relu'))\n    model.add(Dense(8))\n    model.add(Activation('softmax'))\n    print (\"model A : \")\n    model.summary()\n    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n    return model\n\ndef male_female_pitch():\n    model = Sequential()\n    model.add(Conv1D(256, 5, padding='same', input_shape=(40, 1)))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.1))\n    model.add(MaxPooling1D(pool_size=16))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.1))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Flatten())\n    model.add(Dense(8))\n    model.add(Activation('softmax'))\n    opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n    print (\"model B : \")\n    model.summary()\n    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n    return model","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.random.shuffle(data)\nfemale_number_of_samples = female_data.shape[0]\nfemale_training_samples = int(female_number_of_samples * 0.8)\nfemale_validation_samples = int(female_training_samples * 0.33)\nfemale_test_samples = int(female_number_of_samples-(female_training_samples))\nfemale_class_weights = class_weight.compute_class_weight('balanced', np.unique(female_labels_categorical[female_training_samples]),\n                                                  female_labels_categorical[female_training_samples])\nprint (female_training_samples)\nprint (female_validation_samples)\nprint (female_test_samples)\n# print (female_class_weights)\n# male_female_pitch()","execution_count":13,"outputs":[{"output_type":"stream","text":"model B : \nModel: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1d_1 (Conv1D)            (None, 40, 256)           1536      \n_________________________________________________________________\nactivation_10 (Activation)   (None, 40, 256)           0         \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, 40, 128)           163968    \n_________________________________________________________________\nactivation_11 (Activation)   (None, 40, 128)           0         \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 40, 128)           0         \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, 2, 128)            0         \n_________________________________________________________________\nconv1d_3 (Conv1D)            (None, 2, 128)            82048     \n_________________________________________________________________\nactivation_12 (Activation)   (None, 2, 128)            0         \n_________________________________________________________________\nconv1d_4 (Conv1D)            (None, 2, 128)            82048     \n_________________________________________________________________\nactivation_13 (Activation)   (None, 2, 128)            0         \n_________________________________________________________________\nconv1d_5 (Conv1D)            (None, 2, 128)            82048     \n_________________________________________________________________\nactivation_14 (Activation)   (None, 2, 128)            0         \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 2, 128)            0         \n_________________________________________________________________\nconv1d_6 (Conv1D)            (None, 2, 128)            82048     \n_________________________________________________________________\nactivation_15 (Activation)   (None, 2, 128)            0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 256)               0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 8)                 2056      \n_________________________________________________________________\nactivation_16 (Activation)   (None, 8)                 0         \n=================================================================\nTotal params: 495,752\nTrainable params: 495,752\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"<keras.engine.sequential.Sequential at 0x7f6534ba7f60>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"### train using model A\nmodel_A = female_create_model_LSTM()  # male_female_pitch()\nhistory = model_A.fit(np.expand_dims(female_data[:female_training_samples],-1), \n                      female_labels_categorical[:female_training_samples], \n                      validation_data=(np.expand_dims(female_data[female_validation_samples:], -1), \n                                       female_labels_categorical[female_validation_samples:]), \n                      class_weight=female_class_weights, epochs=250, shuffle=True)  # class_weight=female_class_weights, ","execution_count":12,"outputs":[{"output_type":"stream","text":"model A : \nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_3 (LSTM)                (None, 128)               66560     \n_________________________________________________________________\ndense_7 (Dense)              (None, 64)                8256      \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 64)                0         \n_________________________________________________________________\nactivation_7 (Activation)    (None, 64)                0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 32)                0         \n_________________________________________________________________\nactivation_8 (Activation)    (None, 32)                0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 8)                 264       \n_________________________________________________________________\nactivation_9 (Activation)    (None, 8)                 0         \n=================================================================\nTotal params: 77,160\nTrainable params: 77,160\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"},{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"<keras.engine.sequential.Sequential at 0x7f6534e9a3c8>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"### loss plots using model A\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'ro', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### accuracy plots using model A\nplt.clf()                                                \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'ro', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"j = 1\nprint (\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        j + 1, 100, max(history.history['accuracy']), max(history.history['val_accuracy'])))\n# predictions = model_A.predict(np.expand_dims(data[test_samples:], -1))\n# predictions[0:5]\ntrain_acc = model_A.evaluate(np.expand_dims(female_data[:female_training_samples], -1), female_labels_categorical[:female_training_samples], verbose=0)\ntest_acc = model_A.evaluate(np.expand_dims(female_data[female_test_samples:], -1),\n                                female_labels_categorical[female_test_samples:], verbose=0)\nval_acc = model_A.evaluate(np.expand_dims(female_data[female_validation_samples:], -1), female_labels_categorical[female_validation_samples:], verbose=0)\nprint (\"accuracy printing\")\nprint (train_acc)\nprint (test_acc)\nprint (val_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####################male gender detection#################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ### although, we load the data here, it is not used in training or validation\n# for dirname, _, filenames in os.walk('/kaggle/input/audiowav-cremad/AudioWAV_cremad/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\nroot_dir = \"/kaggle/input/audiowav-cremad/AudioWAV_cremad/\" \n\nmale_cremad_only_data = [] ###stores the mfcc data\nmale_cremad_only_labels = [] ###stores the labels\nfor subdirs, dirs, files in os.walk(root_dir):\n    for file in files:\n        if file[0] == 'm':\n            if file[11:14] == 'NEU':\n                    continue\n            else:\n                y, sr = librosa.load(os.path.join(subdirs,file), res_type='kaiser_best', \n                                         duration=librosa.get_duration(filename=os.path.join(subdirs,file)), \n                                     offset=0.25)\n                mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0)\n                male_cremad_only_data.append(mfccs)\n                male_cremad_only_labels.append(classification(file))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nmale_cremad_only_data_array = np.array(male_cremad_only_data)\nmale_cremad_only_labels_array = np.array(male_cremad_only_labels)\nmale_cremad_only_data_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input/ravdessaudiofilessfromvideo/ravdess_audio_files_from_video/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n### extract audio data from AV RAVDESS data\nroot_dir = \"/kaggle/input/ravdessaudiofilessfromvideo/ravdess_audio_files_from_video/\" \n\nmale_audio_only_data = []  # stores the mfcc data\nmale_audio_only_labels = []  # stores the labels\nfor subdirs, dirs, files in os.walk(root_dir):\n    for file in files:\n        if int(file[18:-4]) % 2 == 1:\n            if int(file[7:8]) == 1:\n                    continue\n            if int(file[7:8]) == 2:\n                continue\n            else:\n                y, sr = librosa.load(os.path.join(subdirs, file), res_type='kaiser_best', \n                                 duration=librosa.get_duration(filename=os.path.join(subdirs,file)), \n                                offset=0.5)\n                mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n                male_audio_only_data.append(mfccs)\n                male_audio_only_labels.append(int(file[7:8]) - 1)\n                 #         male_audio_only_labels.append(classification(file))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nmale_audio_only_data_array = np.array(male_audio_only_data)\nmale_audio_only_labels_array = np.array(male_audio_only_labels)\nmale_audio_only_data_array.shape\nmale_audio_only_labels_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### load data from savee dataset\n#### although, we load the data here, it is not used in training or validation\n# for dirname, _, filenames in os.walk('/kaggle/input/savee-database/audiodata/AudioData/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\nroot_dir = \"/kaggle/input/savee-database/audiodata/AudioData/\"\n# root_dir = \"../input/audio_speech_actors_01-24/\"\nmale_savee_data = []\nmale_savee_labels = []\nfor actor_dir in sorted(os.listdir(root_dir)):\n    if actor_dir[-4:] == \".txt\":\n        continue\n    for file_name in os.listdir(os.path.join(root_dir, actor_dir)):\n        if file_name[0] == \"c\":\n            continue\n        if file_name[0] == \"n\":\n#             male_savee_labels.append(0)\n            continue\n        wav_file_name = os.path.join(root_dir, actor_dir, file_name)\n        male_savee_data.append(extract_mfcc(wav_file_name))\n#         male_savee_labels.append(classification(file_name))\n        if file_name[0] == \"a\":\n            male_savee_labels.append(4)\n        if file_name[0] == \"d\":\n            male_savee_labels.append(6)\n        if file_name[0] == \"f\":\n            male_savee_labels.append(5)\n        if file_name[0] == \"h\":\n            male_savee_labels.append(2)\n        if file_name[:2] == \"sa\":\n            male_savee_labels.append(3)\n        if file_name[:2] == \"su\":\n            male_savee_labels.append(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nmale_savee_data_array = np.asarray(male_savee_data)\nmale_savee_label_array = np.array(male_savee_labels)\n# to_categorical(male_savee_label_array)[0].shape\nmale_savee_data_array.shape\nmale_savee_label_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\nroot_dir = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\" \n##### load radvess speech data #####\n# root_dir = \"../input/audio_speech_actors_01-24/\"\n# root_dir = \"../input/audio_speech_actors_01-24/\"\n# actor_dir = os.listdir(\"../input/audio_speech_actors_01-24/\")\nmale_radvess_speech_labels = []\nmale_ravdess_speech_data = []\nfor actor_dir in sorted(os.listdir(root_dir)):\n    actor_name = os.path.join(root_dir, actor_dir)\n    for file in os.listdir(actor_name):\n        if int(file[18:-4]) % 2 == 1:\n            if int(file[7:8]) == 1:\n                continue\n            if int(file[7:8]) == 2:\n                continue\n            else:\n                male_radvess_speech_labels.append(int(file[7:8]) - 1)\n        #         male_radvess_speech_labels.append(classification(file))\n                wav_file_name = os.path.join(root_dir, actor_dir, file)\n                male_ravdess_speech_data.append(extract_mfcc(wav_file_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nmale_ravdess_speech_data_array = np.asarray(male_ravdess_speech_data)\nmale_ravdess_speech_label_array = np.array(male_radvess_speech_labels)\nmale_ravdess_speech_data_array.shape\nmale_ravdess_speech_label_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n### load RAVDESS song data\nroot_dir = \"../input/ravdess-emotional-song-audio/audio_song_actors_01-24/\"\nmale_radvess_song_labels = []\nmale_ravdess_song_data = []\nfor actor_dir in sorted(os.listdir(root_dir)):\n    actor_name = os.path.join(root_dir, actor_dir)\n    for file in os.listdir(actor_name):\n        if int(file[18:-4]) % 2 == 1:\n            if int(file[7:8]) == 1:\n                continue\n            if int(file[7:8]) == 2:\n                continue\n            else:\n                male_radvess_song_labels.append(int(file[7:8]) - 1)\n        #         male_radvess_song_labels.append(classification(file))\n                wav_file_name = os.path.join(root_dir, actor_dir, file)\n                male_ravdess_song_data.append(extract_mfcc(wav_file_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nmale_ravdess_song_data_array = np.asarray(male_ravdess_song_data)\nmale_ravdess_song_label_array = np.array(male_radvess_song_labels)\nmale_ravdess_song_label_array.shape\nmale_ravdess_song_data_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input/ravdess-audio-from-video-song/ravdess_audio_files_from_video_song/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n\nroot_dir = \"/kaggle/input/ravdess-audio-from-video-song/ravdess_audio_files_from_video_song/\" \n\nmale_audio_only_data_song = [] ###stores the mfcc data\nmale_audio_only_labels_song = [] ###stores the labels\nfor subdirs, dirs, files in os.walk(root_dir):\n    for file in files:\n        if int(file[18:-4]) % 2 == 1:\n            if int(file[7:8]) == 1:\n                continue\n            if int(file[7:8]) == 2:\n                continue\n            else:\n                y, sr = librosa.load(os.path.join(subdirs,file), res_type='kaiser_best', \n                                     duration=librosa.get_duration(filename=os.path.join(subdirs,file)), \n                                     offset=0.5)\n                mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0)\n                male_audio_only_data_song.append(mfccs)\n        #         male_audio_only_labels_song.append(classification(file))\n                male_audio_only_labels_song.append(int(file[7:8]) - 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### convert data to array and make labels categorical\nmale_audio_only_data_song_array = np.array(male_audio_only_data_song)\nmale_audio_only_labels_song_array = np.array(male_audio_only_labels_song)\nmale_audio_only_data_song_array.shape\nmale_audio_only_labels_song_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #### combine data\nmale_data = np.r_[male_audio_only_data_array, male_cremad_only_data_array, male_audio_only_data_song_array, male_ravdess_song_data_array, male_savee_data_array, male_ravdess_speech_data_array]  # ravdess_song_data_array, audio_only_data_song_array,\nmale_labels = np.r_[male_audio_only_labels_array, male_cremad_only_labels_array, male_audio_only_labels_song_array, male_ravdess_song_label_array, male_savee_label_array, male_ravdess_speech_label_array]  # ravdess_song_label_array, audio_only_labels_song_array,\n# male_data = ravdess_speech_data_array\n# male_labels = ravdess_speech_label_array\nmale_labels.shape\nmale_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### make categorical labels\nmale_labels_categorical = to_categorical(male_labels)\nmale_data.shape\nmale_labels_categorical.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def male_create_model_LSTM():\n    ### LSTM model, referred to the model A in the report\n    model = Sequential()\n    model.add(LSTM(128, return_sequences=False, input_shape=(40, 1)))\n    model.add(Dense(64))\n#     model.add(Dropout(0.4))\n    model.add(Activation('relu'))\n    model.add(Dense(32))\n    model.add(Dropout(0.1))   ### 0.4\n    model.add(Activation('relu'))\n    model.add(Dense(8))\n    model.add(Activation('softmax'))\n    print (\"model C : \")\n    model.summary()\n    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n    return model\n\ndef male_female_pitch():\n    model = Sequential()\n    model.add(Conv1D(256, 5, padding='same', input_shape=(40, 1)))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.1))\n    model.add(MaxPooling1D(pool_size=16))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.1))\n    model.add(Conv1D(128, 5, padding='same', ))\n    model.add(Activation('relu'))\n    model.add(Flatten())\n    model.add(Dense(8))\n    model.add(Activation('softmax'))\n    opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n    print (\"model E : \")\n    model.summary()\n    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.random.shuffle(data)\nmale_number_of_samples = male_data.shape[0]\nmale_training_samples = int(male_number_of_samples * 0.8)\nmale_validation_samples = int(male_training_samples * 0.33)\nmale_test_samples = int(male_number_of_samples-(male_training_samples))\nmale_class_weights = class_weight.compute_class_weight('balanced', np.unique(male_labels_categorical[male_training_samples]),\n                                                  male_labels_categorical[male_training_samples])\nprint (male_training_samples)\nprint (male_validation_samples)\nprint (male_test_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### train using model A\nmodel_C = male_female_pitch()  # male_create_model_LSTM()\nhistory = model_C.fit(np.expand_dims(male_data[:male_training_samples],-1), \n                      male_labels_categorical[:male_training_samples], \n                      validation_data=(np.expand_dims(male_data[male_validation_samples:], -1), \n                      male_labels_categorical[male_validation_samples:]),\n                      class_weight=male_class_weights, epochs=400, shuffle=True)  # class_weight=class_weights, ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### loss plots using model A\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'ro', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### accuracy plots using model A\nplt.clf()                                                \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'ro', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"j = 1\nprint (\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        j + 1, 100, max(history.history['accuracy']), max(history.history['val_accuracy'])))\n# predictions = model_A.predict(np.expand_dims(data[test_samples:], -1))\n# predictions[0:5]\ntrain_acc = model_C.evaluate(np.expand_dims(male_data[:male_training_samples], -1), male_labels_categorical[:male_training_samples], verbose=0)\ntest_acc = model_C.evaluate(np.expand_dims(male_data[male_test_samples:], -1),\n                                male_labels_categorical[male_test_samples:], verbose=0)\nval_acc = model_C.evaluate(np.expand_dims(male_data[male_validation_samples:], -1), male_labels_categorical[male_validation_samples:], verbose=0)\nprint (\"accuracy printing\")\nprint (train_acc)\nprint (test_acc)\nprint (val_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### train using model A\ncom_data = np.r_[male_data[:male_training_samples], female_data[:female_training_samples]]\ncom_labels = np.r_[male_labels_categorical[:male_training_samples], female_labels_categorical[:female_training_samples]]\ncom_labels.shape\ncom_data.shape\n\n# com_labels_categorical = to_categorical(com_labels)\n# com_labels_categorical.shape\n\nmodel_check = female_create_model_LSTM()\nhistory = model_check.fit(np.expand_dims(com_data[:male_training_samples+female_training_samples],-1), \n                      com_labels[:male_training_samples+female_training_samples], \n                      validation_split=0.20, epochs=450)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### loss plots using model A\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'ro', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\n### accuracy plots using model A\nplt.clf()                                                \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'ro', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"j = 1\nprint (\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        j + 1, 100, max(history.history['accuracy']), max(history.history['val_accuracy'])))\n# predictions = model_A.predict(np.expand_dims(data[test_samples:], -1))\n# predictions[0:5]\ntrain_acc = model_check.evaluate(np.expand_dims(com_data[:male_training_samples+female_training_samples],-1), \n                      com_labels[:male_training_samples+female_training_samples], verbose=0)\ntest_acc = model_check.evaluate(np.expand_dims(com_data[:male_training_samples+female_training_samples],-1), \n                      com_labels[:male_training_samples+female_training_samples], verbose=0)\nval_acc = model_check.evaluate(np.expand_dims(com_data[:male_training_samples+female_training_samples],-1), \n                      com_labels[:male_training_samples+female_training_samples], verbose=0)\nprint (\"accuracy printing\")\nprint (train_acc)\nprint (test_acc)\nprint (val_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.models import Model\n\n# mergedOutput = Concatenate()([model_A.output, model_C.output])\n# out = Dense(8, activation='softmax')(mergedOutput)\n# model = Model([model_A.input, model_C.input], out)\n# print(model.summary())\n# model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n\n\nfinal_data = np.r_[male_audio_only_data_array, female_audio_only_data_array, male_cremad_only_data_array, female_cremad_only_data_array, male_audio_only_data_song_array, female_audio_only_data_song_array, male_ravdess_song_data_array, female_ravdess_song_data_array, male_savee_data_array, male_ravdess_speech_data_array, female_ravdess_speech_data_array]\nfinal_labels = np.r_[male_audio_only_labels_array, female_audio_only_labels_song_array, male_cremad_only_labels_array, female_cremad_only_labels_array, male_audio_only_labels_song_array, female_audio_only_labels_array, male_ravdess_song_label_array, female_ravdess_song_label_array, male_savee_label_array, male_ravdess_speech_label_array, female_ravdess_speech_label_array]\n\nfinal_labels_categorical = to_categorical(final_labels)\nfinal_data.shape\nfinal_labels_categorical.shape\n\nfinal_number_of_samples = final_data.shape[0]\nfinal_training_samples = int(final_number_of_samples * 0.8)\nfinal_validation_samples = int(final_training_samples * 0.33)\nfinal_test_samples = int(final_number_of_samples-(final_training_samples))\nfinal_class_weights = class_weight.compute_class_weight('balanced', np.unique(final_labels_categorical[final_training_samples]),\n                                                  final_labels_categorical[final_training_samples])\nprint (final_training_samples)\nprint (final_validation_samples)\nprint (final_test_samples)\n\n# #### combine data\n# com_data = np.r_[male_data[:male_training_samples], female_data[:female_training_samples]]\n# com_labels = np.r_[male_labels_categorical[:male_training_samples], female_labels_categorical[:female_training_samples]]\n# com_labels.shape\n# com_data.shape\n# np.expand_dims(com_data[:male_training_samples+female_training_samples],-1)\n\n# # final_model = male_female_pitch()  # male_create_model_LSTM()\n# x = (np.expand_dims(male_data[male_training_samples]+female_data[female_training_samples],-1))\n# print (x)\n# y = (male_labels_categorical[:male_training_samples]+female_labels_categorical[:female_training_samples])\n# y1 = without_gender_labels_categorical[without_gender_training_samples]\n# print(y)\n\n# history = model.fit([np.expand_dims(male_data[:male_training_samples],-1), np.expand_dims(female_data[:female_training_samples],-1)], \n#                       [male_labels_categorical[:male_training_samples], female_labels_categorical[:female_training_samples]]) \n\n# [np.expand_dims(male_data[:male_training_samples],-1), np.expand_dims(female_data[:female_training_samples],-1)],y\n\n# history = model.fit([np.expand_dims(com_data[:male_training_samples+female_training_samples],-1), np.expand_dims(com_data[:male_training_samples+female_training_samples],-1)],\n#                     com_labels[:male_training_samples+female_training_samples], validation_split = 0.33\n#                     epochs=400)\n\nmodel = male_female_pitch()  # male_create_model_LSTM()\nhistory = model.fit(np.expand_dims(final_data[:final_training_samples],-1), \n                      final_labels_categorical[:final_training_samples], \n                      validation_data=(np.expand_dims(final_data[final_validation_samples:], -1), \n                      final_labels_categorical[final_validation_samples:]),\n                      class_weight=final_class_weights, epochs=800, shuffle=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### loss plots using model A\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'ro', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\n### accuracy plots using model A\nplt.clf()                                                \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'ro', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"j = 1\nprint (\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        j + 1, 100, max(history.history['accuracy']), max(history.history['accuracy'])))\n# predictions = model_A.predict(np.expand_dims(data[test_samples:], -1))\n# predictions[0:5]\ntrain_acc = model.evaluate(np.expand_dims(final_data[:final_training_samples], -1), final_labels_categorical[:final_training_samples], verbose=0)\ntest_acc = model.evaluate(np.expand_dims(final_data[final_test_samples:], -1),\n                                final_labels_categorical[final_test_samples:], verbose=0)\nval_acc = model.evaluate(np.expand_dims(final_data[final_validation_samples:], -1), final_labels_categorical[final_validation_samples:], verbose=0)\nprint (\"accuracy printing\")\nprint (train_acc)\nprint (test_acc)\nprint (val_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### plot a histogram to understand the distribution of the data\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\n\nimport collections\n\n# sizes = collections.Counter(labels)\n# print (sizes)\n\n# plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n# x = np.arange(8)\n# colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange', 'grey', 'brown']\n# plt.hist(labels, color = colors, normed=False, bins=20) \n# plt.xticks(x, ['neutral','calm','happy', 'sad', 'angry', 'fear', 'disgust', 'surprised'])\n# # plt.hist(labels)\n# plt.show()\n\n# fig, ax = plt.subplots()\n\n# N, bins, patches = ax.hist(labels, edgecolor='white', linewidth=1, normed=True, bins=20)\n\n# colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange', 'grey', 'brown']\n# for i in range(0, 1):\n#     patches[i].set_facecolor('r')\n# for i in range(1, 3):\n#     patches[i].set_facecolor('yellow')\n# for i in range(3, 4):\n#     patches[i].set_facecolor('g')\n\n# plt.xticks(x, ['neutral','calm','happy', 'sad', 'angry', 'fear', 'disgust', 'surprised'])\n# plt.show()\n\n\n# 01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised)\n# Counter({4: 1307, 2: 1307, 5: 1307, 3: 1307, 6: 1019, 0: 983, 1: 576, 7: 348})\n# 0: 12% 1: 7% 2: 16% 3: 16% 4: 16% 5: 16% 6: 13% 7: 4%\nfemale_labels = [r'happy(20%):1,152', r'sad(20%):1,152', r'angry(20%):1,152', r'fear(20%):1,152', r'disgust(15%):888', r'surprised(5%):288']  # r'neutral', r'calm', \nmale_labels = [r'happy(20%):1,307', r'sad(20%):1,307', r'angry(20%):1,307', r'fear(20%):1,307', r'disgust(15%):1,019', r'surprised(5%):348']\ncolors = ['red', 'blue', 'green', 'yellow', 'purple', 'cyan']\nfemale_sizes = [1152, 1152, 1152, 1152, 888, 288]\nmale_sizes = [1307, 1307, 1307, 1307, 1019, 348]  # 983, 576, \npatches, texts = plt.pie(female_sizes, colors=colors, startangle=90)\nplt.legend(patches, female_labels, loc=\"best\")\n# Set aspect ratio to be equal so that pie is drawn as a circle.\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n\npatches, texts = plt.pie(male_sizes, colors=colors, startangle=90)\nplt.legend(patches, male_labels, loc=\"best\")\n# Set aspect ratio to be equal so that pie is drawn as a circle.\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n\ngender_labels = [r'male gender 53.27%(6595)', r'female gender 46.73%(5784)']\ngender_sizes = [(6595/12379)*100, (5784/12379)*100]\ngender_colors = ['red', 'blue']\npatches, texts = plt.pie(gender_sizes, colors=gender_colors, startangle=90)\nplt.legend(patches, gender_labels, loc=\"best\")\n# Set aspect ratio to be equal so that pie is drawn as a circle.\nplt.axis('equal')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}